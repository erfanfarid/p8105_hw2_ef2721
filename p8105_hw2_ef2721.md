p8105_hw2_ef2721
================
Erfan Faridmoayer

``` r
library(tidyverse)
library(readxl)
```

## Problem 1

Below we import and clean data from
`NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. First data is imported,
then the `Route` column names that are not `dbl` are turned into `chr`.
We then use janitor function, select the noted columns, and change
`entry` from `yes` / `no` to a logical variable.

``` r
trans_df = 
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>%
  janitor::clean_names() %>% 
  select(
    line, station_name, station_latitude, station_longitude, 
    starts_with("route"), entry, exit_only, vending, entrance_type, 
    ada) %>% 
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

The presentation is not yet “tidy”: route number should be a variable,
as should route. That is, to obtain a tidy dataset we would need to
convert `route` variables from wide to long format. This will be useful
when focusing on specific routes, but may not be necessary when
considering questions that focus on station-level variables.

The following code chunk selects station name and line, and then uses
`distinct()` to obtain all unique combinations. As a result, the number
of rows in this dataset is the number of unique stations.

``` r
trans_df %>% 
  select(station_name, line) %>% 
  distinct
## # A tibble: 465 × 2
##    station_name             line    
##    <chr>                    <chr>   
##  1 25th St                  4 Avenue
##  2 36th St                  4 Avenue
##  3 45th St                  4 Avenue
##  4 53rd St                  4 Avenue
##  5 59th St                  4 Avenue
##  6 77th St                  4 Avenue
##  7 86th St                  4 Avenue
##  8 95th St                  4 Avenue
##  9 9th St                   4 Avenue
## 10 Atlantic Av-Barclays Ctr 4 Avenue
## # … with 455 more rows
## # ℹ Use `print(n = ...)` to see more rows
```

There are 465 unique stations based on the code chunk above.

To identify unique stations that are `ada` complaints, we first filter
the data from ada that are `TRUE`. Then, similar to above, we select
based on station name and line. Below is the code chunk explanation.

``` r
trans_df %>% 
  filter(ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
## # A tibble: 84 × 2
##    station_name                   line           
##    <chr>                          <chr>          
##  1 Atlantic Av-Barclays Ctr       4 Avenue       
##  2 DeKalb Av                      4 Avenue       
##  3 Pacific St                     4 Avenue       
##  4 Grand Central                  42nd St Shuttle
##  5 34th St                        6 Avenue       
##  6 47-50th Sts Rockefeller Center 6 Avenue       
##  7 Church Av                      6 Avenue       
##  8 21st St                        63rd Street    
##  9 Lexington Av                   63rd Street    
## 10 Roosevelt Island               63rd Street    
## # … with 74 more rows
## # ℹ Use `print(n = ...)` to see more rows
```

There are 84 compliant stations.

For the next sub-question, we first filter the stations with no vending
machine. We then see how many stations from that subset allow entry to
be possible. Mean is proportion here since entry is a logical variable.

``` r
trans_df %>% 
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean
## [1] 0.3770492
```

the proportion desired was 0.38.

We use the `pivot_longer` function to reformat the data to add route
number and route name as distinct variables. Subsequently, we use the
`filter` function to narrow down which stations service A line, and of
those which are ADA compliant, respectively. The code chunk below
addresses that

``` r
trans_df %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>% 
  select(station_name, line) %>% 
  distinct
## # A tibble: 60 × 2
##    station_name                  line           
##    <chr>                         <chr>          
##  1 Times Square                  42nd St Shuttle
##  2 125th St                      8 Avenue       
##  3 145th St                      8 Avenue       
##  4 14th St                       8 Avenue       
##  5 168th St - Washington Heights 8 Avenue       
##  6 175th St                      8 Avenue       
##  7 181st St                      8 Avenue       
##  8 190th St                      8 Avenue       
##  9 34th St                       8 Avenue       
## 10 42nd St                       8 Avenue       
## # … with 50 more rows
## # ℹ Use `print(n = ...)` to see more rows
```

There are 60 distinct stations that serve the A line

``` r
trans_df %>% 
  pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
## # A tibble: 17 × 2
##    station_name                  line            
##    <chr>                         <chr>           
##  1 14th St                       8 Avenue        
##  2 168th St - Washington Heights 8 Avenue        
##  3 175th St                      8 Avenue        
##  4 34th St                       8 Avenue        
##  5 42nd St                       8 Avenue        
##  6 59th St                       8 Avenue        
##  7 Inwood - 207th St             8 Avenue        
##  8 West 4th St                   8 Avenue        
##  9 World Trade Center            8 Avenue        
## 10 Times Square-42nd St          Broadway        
## 11 59th St-Columbus Circle       Broadway-7th Ave
## 12 Times Square                  Broadway-7th Ave
## 13 8th Av                        Canarsie        
## 14 Franklin Av                   Franklin        
## 15 Euclid Av                     Fulton          
## 16 Franklin Av                   Fulton          
## 17 Howard Beach                  Rockaway
```

There are 17 distinct stations which serve the A line and are ADA
compliant.

## Problem 2

The `readxl` library has been introduced earlier in the code. In this
problem, we first read the data, narrowing to the sheet desired. To
avoid unnecessary images, the data were selected to the numerical table.

We then use the `janitor` function to clean the data, and selected
dumpster-specific data. Using the `filter` function, the rows with
summary values were removed. The `as.integer` function was used to round
sports_balls values to the nearest round number. Lastly, `mutate` was
used to move the column with titles to the beginning of the data set,
and to change `year` to the *double* variable.

Below, is the code chunk implementing above:

``` r
trash_df =
  read_excel("./data/Trash Wheel Collection Data.xlsx", 
      sheet = "Mr. Trash Wheel", 
      range = "A2:N549") %>% 
  janitor::clean_names() %>% 
  select(-homes_powered) %>% 
  drop_na %>% 
  mutate(
    sports_balls = as.integer(sports_balls)
  ) %>% 
  mutate(
    dumpster = as.integer(dumpster)
  ) %>% 
  mutate(
    trash_source = "mr_trash_wheel",
    .before = dumpster) %>% 
  mutate(
    year = as.double(year)
  )
```

Of note, add the end of the code-chunk above, the `mutate` function was
used to add a column at the beginning of the dataframe to distinguish
source of data. We will now use a similar approach to import and clean
the data from “Professor Trash Wheel”. Please see the code-chunk below:

``` r
protrash_df = 
  read_excel("./data/Trash Wheel Collection Data.xlsx",
      sheet = "Professor Trash Wheel",
      range = "A2:M96") %>%
  janitor::clean_names() %>%
  select(-homes_powered) %>%
  drop_na %>% 
  mutate(
    dumpster = as.integer(dumpster)
  ) %>% 
  mutate(
    trash_source = "prof_trash_wheel",
    .before = dumpster)
```

In each dataframe, the “dumpster” variable contained different types. As
such, both were turned into integers using `as.integer` function.

Now, both data frames are ready to be added together. We will do so in
the code chunk below:

``` r
alltrash_df =
  bind_rows(trash_df, protrash_df) %>% 
  janitor::clean_names()
```

The result of the code chinks above is a dataframe called “alltrash_df”,
which was created using the `readxl` package. There are a total of
**641** observations in this dataframe, **547** of which belong to the
*Mr. Trash Wheel* dataset. The average weight of the trash in *Mr. Trash
Wheel* was **3.1962706 tons** and, in *Professor Trash Wheel* was
**2.0225532 tons**. The total weight of trash collected by *Professor
Trash Wheel* was **190.12 tons**. The total number of sports balls
collected by *Mr. Trash Wheel* in 2020 was **856 balls**.

## Problem 3

### Part 1

In the code chunk below, we imported the *pols-month* dataset into the
`pol_df` dataframe. We then cleaned it up using `janitor`, then split
the dates into their year, month, and day components using `separate()`
argument, splitting by *“-”*. To assure desired arrangement, `arrange()`
was used to order dataframe based on year and month.

The month number was then converted to written using `month.name`
function. Afterward, the president column was created with `mutate()`
utilizing the `prez_gop` and `prez_dem` columns. The `if_else` function
was used to define values of the two available conditions, using `gop`
and `dem` values, respectively.

Lastly, the `prez_gop`, `prez_dem`, and `day` columns were removed using
the `select()` function.

``` r
pol_df = 
  read_csv("./data/fivethirtyeight_datasets/pols-month.csv") %>% 
  janitor::clean_names() %>% 
  separate(
    mon, 
    into = c("year", "month", "day"), 
    sep = "-", 
    convert = TRUE) %>% 
  arrange(year, month) %>% 
  mutate(month = month.name[month]) %>% 
  mutate(president = prez_gop - prez_dem) %>% 
  mutate(president = if_else(president <0, "dem", "gop")) %>% 
  select(-prez_gop, -prez_dem, -day) %>% 
  select(year, month, president, everything())
```

### Part 2

Next, in the following code chunk, we imported and cleaned the `snp.csv`
file using the `read_csv` and `janitor::clean_names()` functions. we
then separated the date into 3 columns.

The order here, however, was different than the previous chunk. So, we
followed an *MDY* model. In order to change the dates from 2-digits into
4 digits, an additional conditional variable was created, and `mutate()`
function was used to differentiate centuries based on relation to
current year 22.

The columns were then sorted by year and month using `arrange()`
function. For consistency, month was changed from numerical to written
description. Lastly, using `select()`, the intermediate variables and
date columns. were removed.

``` r
snp_df =
  read_csv("./data/fivethirtyeight_datasets/snp.csv") %>% 
  janitor::clean_names() %>% 
  separate(
    date, 
    into = c("month", "date", "dig_2_year"), 
    sep = "/", 
    convert = TRUE) %>% 
  mutate(alter = if_else(dig_2_year < 22, "2000", "1900")) %>% 
  mutate(alter = as.integer(alter)) %>% 
  mutate(year = alter + dig_2_year, .before = month) %>% 
  arrange(year, month) %>% 
  mutate(month = month.name[month]) %>% 
  select(-dig_2_year, -alter, -date)
```

### Part 3

Third, tidy the unemployment data so that it can be merged with the
previous datasets. This process will involve switching from “wide” to
“long” format; ensuring that key variables have the same name; and
ensuring that key variables take the same values.

In this code-chunk, the *unemployment* database was first read by the
`read.csv` function, placed into the *unemp_df* dataframe, and was then
cleaned using the `janitor` package. Subsequently, the *unemp_tidy_df*
dataframe was introduced to utilize the `pivot_longer` function, to
associate the months with each year through the new `month` variable and
the unemployment rates to the new `unemployment_rate` column.

Lastly, for consistency, `mutate` function was used to capitalize the
first month letters, followed by the `month.name` function to to expand
the 3-letter `month.abb` values. In the last line, rows without values
were removed using `drop_na()` function.

``` r
unemp_df = 
  read.csv("./data/fivethirtyeight_datasets/unemployment.csv") %>% 
  janitor::clean_names()

unemp_tidy_df = 
  pivot_longer(
    unemp_df, 
    jan:dec,
    names_to = "month", 
    values_to = "unemployment_rate") %>%
    mutate(month = str_to_title(month)) %>% 
    mutate(month = month.name[match(month, month.abb)]) %>% 
    drop_na()
```

Next, we use the `left-join` function to merge the `snp_df` into the
`pol_df` dataframes, creating results in a new `pol_snp_df` dataframe.
Finally, we will merge the `unemp_tidy_df` into `pol_snp_df`, saving the
results in the new dataframe `pol_snp_unemp_df`. All the merges were
conducted by `year` and `month` variables.

``` r
pol_snp_df = 
  left_join(pol_df, snp_df, by = c("year", "month"))

pol_snp_unemp_df =
  left_join(pol_snp_df, unemp_tidy_df, by = c("year", "month") ) %>% 
  drop_na()
```

Given the different range of years where data was collected, as a final
tidy step, we will use the `drop_na` function to remove the absent
datapoints.

### Discussion

The following datasets were utilized:

-   `pol_df` contains information on the political party affiliations in
    offices of presidency, senate, congress, and governorship between
    years of **1947** and **2015**. It contains **822 rows** and **9
    columns**.
-   `snp_df` contains information about the closing *S&P* stock value at
    different months between years of **1950** and **2015**. It contains
    **787 rows** and **3 columns**.
-   `unemp_tidy_df` contains information on *unemployment rate* between
    years of **1948** and **2015**. It contains **810 rows** and **3
    columns**.
-   `pol_snp_unemp_df` is the collective database made from the three
    above, showcasing political party at large between years of **1950**
    and **2015**, the *S&P* stock value at those times, as well as the
    *unemployment rate* of those time periods. It contains **786 rows**
    and **11 columns**.
